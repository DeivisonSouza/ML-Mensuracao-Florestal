---
title: "Aprendizado de M√°quina <BR> na Mensura√ß√£o Florestal:"
subtitle: "T√©cnicas, Modelagem e Aplica√ß√£o Web  <BR> <BR> <BR> <BR>"
#subtitle: "Potencial para modelagem de vari√°veis florestais"
author: "<BR> Prof. Dr. Deivison Venicio Souza"
institute: |
  | Universidade Federal do Par√° (UFPA) <BR> <BR> 
date: | 
  .center[`r format(Sys.Date(),"%d/%B/%Y")` <BR> 
   Altamira, Par√° <BR> ]
encoding: "UTF-8"
#output: revealjs::revealjs_presentation
#output: rmdshower::shower_presentation
header-includes: 
   - \usepackage[brazil]{babel}
   - \usepackage{tikz}
   - \usepackage[hidelinks,pdfencoding=auto]{hyperref}
   - \usepackage{smartdiagram}
output:
  xaringan::moon_reader:
    includes:
      after_body: css/sydney.css
    css: [default, metropolis] #xaringan-themer.css 
    lib_dir: libs
    nature:
      #autoplay: 30000
      #countdown: 60000
      #titleSlideClass: [top, left, inverse]
      highlightStyle: idea #monokai
      highlightLines: true
      highlightLanguage: r
      countIncrementalSlides: false
      ratio: "16:9"
    seal: true
---

name: toc

```{r setup, include=FALSE, cache=FALSE}
library(tidyverse)
library(rpart)                            
library(rpart.plot)
library(kableExtra)

options(htmltools.dir.version = FALSE, 
        knitr.graphics.auto_pdf = TRUE,
        servr.daemon = TRUE)

knitr::opts_chunk$set(
  fig.showtext = TRUE,
  fig.align = "center", 
  cache = TRUE,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE ,
  dpi = 600,
  dev.args=list(bg='transparent')
)

```

```{r xaringanExtra, echo=FALSE}
library(xaringanExtra)
# use_logo(
#   image_url = "fig/ufpa.png",
#   position = css_position(top = ".8em", right = "1em"),
#   width = "140px",
#   height = "140px"
# )
# 
use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)

# use_tachyons()
use_panelset()
# use_clipboard()
#style_panelset(panel_tab_color_active = "red")
```

```{r xaringanthemer, warning=FALSE, include=FALSE}
library(xaringanthemer)
style_mono_accent(
  base_color = "#43418A",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Fira Mono")
)
```

```{r, load_refs, include=FALSE, cache=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           cite.style = "authoryear",
           first.inits = TRUE,
           style = "html",
           hyperlink = FALSE,
           dashed = FALSE)
(myBib <- ReadBib("./bib/ref.bib", check = FALSE))
```

# üëã Ol√°!

## <bdi style="color:magenta;">1. Deivison Souza </bdi>

.pull-left[
* <bdi style="font-weight:bold">Gradua√ß√£o (Titula√ß√£o: ano 2008)</bdi>
    + Universidade Federal Rural da Amaz√¥nia (UFRA); e
    + T√≠tulo: Bacharel em Engenharia Florestal.

* <bdi style="font-weight:bold">Mestrado (Titula√ß√£o: ano 2011)</bdi>
    + Universidade Federal Rural da Amaz√¥nia (UFRA);
    + Programa de P√≥s-gradua√ß√£o em Ci√™ncias Florestais (PPGCF); e
    + √Årea de Concentra√ß√£o: Manejo de ecossistemas florestais.
]

.pull-right[
* <bdi style="font-weight:bold">Doutorado (Titula√ß√£o: ano 2020)</bdi>
    + Universidade Federal do Paran√° (UFPR);
    + Programa de P√≥s-gradua√ß√£o em Engenharia Florestal (PPGEF); e
    + √Årea de Concentra√ß√£o: Manejo Florestal.
    
* <bdi style="font-weight:bold">Especializa√ß√£o (Defesa: ano 2019)</bdi>
    + Universidade Federal do Paran√° (UFPR);
    + √Årea: Big Data e Data Science
]

<!-- Slide 2 -->
---
background-image: url(fig/white.jpg)
background-size: cover
# Conte√∫do

.pull-top[
**Parte 1 - Conceitos e fundamenta√ß√£o te√≥rica**

[1 - Qual √© o motivo de fazer an√°lise de regress√£o?](#AR)

[2 - Culturas na modelagem estat√≠stica](#CultME)

[3 - Aprendizado de M√°quina](#AM)

&nbsp;&nbsp;[3.1 - Tipos de Aprendizagem](#TA)

&nbsp;&nbsp;[3.2 - Algoritmos de Aprendizado Supersivionado](#AAS)

&nbsp;&nbsp;&nbsp;&nbsp;[3.2.1 - Regression Tree](#RT)

&nbsp;&nbsp;&nbsp;&nbsp;[3.2.2 - Bagging](#bag)

[4 - Aprendizado de M√°quina com linguagem R](#AMLR)

[5 - Pacote CARET](#caret)
]

<!-- Slide 2 -->
---
background-image: url(fig/white.jpg)
background-size: cover
# Conte√∫do

.pull-top[
**Parte 2 - Aplica√ß√µes nas Ci√™ncias Florestais**

[1 - Aplica√ß√µes nas ci√™ncias florestais](#aplicacoes)

[2 - Aprendizado de M√°quina no R: exemplo pr√°tico](#exemploR)

[3 - Aplica√ß√£o Web com MAM: um exemplo pr√°tico.](#shiny)
]

<!-- Slide XX -->
---
name: AR
class: inverse, middle, left
background-image: url(fig/white.jpg)
background-size: cover

.font200[**Qual √© o motivo de fazer <br> an√°lise de regress√£o?**]

<!--Slide 3 -->
---
# Uma an√°lise de regress√£o...

.pull-left-2[
.blockquote[
.center[**Motivos pr√°ticos**]

- **Determina√ß√£o**: Vari√°veis dif√≠ceis de determinar em campo;
  - ex.: volume, biomassa, altura, etc.
- **Alto custo**: Almeja-se diminuir custos!
- **Larga escala**: Imagine fazer isso em larga escala! √â impratic√°vel!
- **Alternativa**: Usar m√©todos indiretos.
]
]

.pull-right-1[
```{r, echo=FALSE, out.width='50%', fig.align='center', fig.cap='', dpi=600}
knitr::include_graphics('fig/imgCub.jpg')
```
<br>
.center[**Nas florestas inequi√¢neas as dificuldades octuplicam!**]
]

**Regress√£o Linear**: M√©todo mais tradicional!

$v = \beta_0 + \beta_{1}(d^2h) + \epsilon_i$ $\Rightarrow$ **Spurr (1952)**

$ln (v) = ln\beta_0 + \beta_{1}ln(d) + \beta_{2}ln(h) + \epsilon_i$ $\Rightarrow$ **Schumacher-Hall (1933)**

<!--Slide 4 -->
---
# Uma an√°lise de regress√£o...

.blockquote[
.center[**Motivos estat√≠sticos** `r Citep(myBib, "izbicki_mendonca2020")`]

- **Inferencial**: Tirar conclus√µes sobre a popula√ß√£o.
  - Interpretar par√¢metros;
  - Estimar os intervalos de confian√ßa (IC) dos par√¢metros; e
  - Testes de hip√≥teses (normalidade, homocedasticidade dos res√≠duos, etc.) $\Rightarrow$ .content-box-red[Suposi√ß√µes da RL]

*Por exemplo:* Admitindo o modelo $v = \beta_0 + \beta_1d + \epsilon_i$ (**Berkhout**)

Qual a rela√ß√£o entre $v$ e o $d$?

Qual efeito no volume para uma unidade de mudan√ßa no di√¢metro da √°rvore?

- **Preditivista**: 
  - Estimar uma fun√ß√£o de regress√£o $f(x)$ com bom poder preditivo! .content-box-red[O foco √© na predi√ß√£o!]
]

--

.content-box-blue[
.center[**Portanto, para um determinado problema o foco pode ser claramente inferencial ou preditivo, e para outros pode ser a mistura de ambos** `r Citep(myBib, "izbicki_mendonca2020")`.]
]

<!-- Slide XX -->
---
name: CultME
class: inverse, middle, left
background-image: url(fig/white.jpg)
background-size: cover

.font200[**Culturas na modelagem <br> estat√≠stica**]

<!--Slide 5 -->
---
# As duas culturas...`r Citep(myBib, "breiman2001statistical")`

.pull-left-2[
.blockquote[

- **Data modeling culture**
  - *Objetivo principal*: .content-box-red[infer√™ncia] (interpretabilidade)
  - *Predomina*: comunidade estat√≠stica
  - *Popula√ß√£o estimada*: 98% de todos os estat√≠sticos
  - *Forma funcional*: suposi√ß√£o da forma funcional $f(x)$ 
  - *Par√¢metros*: s√£o estimados a partir dos dados
]

.blockquote[
- **Algorithmic modeling culture**
  - *Objetivo principal*: .content-box-red[predi√ß√£o] (poder preditivo!)
  - *Predomina*: comunidade de aprendizado de m√°quina
  - *Popula√ß√£o estimada*: 2% de todos os estat√≠sticos
  - *Forma funcional*: $f(x)$ √© desconhecida e complexa
]
]

.pull-right-1[
```{r, echo=FALSE, out.width='70%', fig.align='center', fig.cap='', dpi=600}
knitr::include_graphics('fig/Breiman.jpg')
```
<br>
.center[**Leo Breiman**]
.center[Fonte: [en.wikipedia.org](https://en.wikipedia.org/wiki/Leo_Breiman)]
]

.content-box-blue[.center[Statistical modeling: The two cultures `r Citep(myBib, "breiman2001statistical")`.]
]

<!-- Slide XX -->
---
name: AM
class: inverse, middle, left
background-image: url(fig/white.jpg)
background-size: cover

.font200[**Aprendizado de M√°quina** <br> (*Machine Learning*)]

<!--Slide 6 -->
---
# Aprendizado de M√°quina (*Machine Learning*)

**Hist√≥ria:**
- Surgiu em 1959;
- Arthur Lee Samuel: Cunhou pela primeira vez o termo *Machine Learning*; e
- Manuscrito: .blue[*Some Studies in Machine Learning Using the Game of Checkers*] `r Citep(myBib, "samuel1959some", .opts = list(hyperlink = "to.doc", super = TRUE))`.

--

.pull-left[
.blockquote[
**Defini√ß√£o:** <br>
√â um .content-box-red[subconjunto de intelig√™ncia artificial] que frequentemente usa t√©cnicas estat√≠sticas para dar aos computadores a capacidade de .content-box-red["aprender"] com dados, sem serem explicitamente programados. <br>

.tr[
‚Äî (Parafraseado de Arthur Lee Samuel, 1959)
]
]
]

.pull-right[
Arthur Samuel demonstrates how machine <br> learning can be used to play checkers in 1962.
```{r, echo=FALSE, out.width='80%', fig.align='left', fig.cap='', dpi=600}
knitr::include_graphics('fig/ArtSam.jpg')
```
.left[.footnote[Fonte: [artsandculture.google.com](https://artsandculture.google.com/asset/arthur-samuel-demonstrates-how-machine-learning-can-be-used-to-play-checkers-in-1962-ibm-watson-media/vgH46mpas8dL4g)]]
]

<!--Slide 7 -->
---
# Aprendizado de M√°quina (*Machine Learning*)

.pull-left[
.blockquote[
**Tom Mitchel**<br>
Pode-se afirmar que um programa de computador aprende uma
**Experi√™ncia E** a respeito de uma **Tarefa T** e alguma medida de **Desempenho P**, se seu desempenho em *T*, medido por *P*, melhora com a experi√™ncia *E*.
]
]

.pull-right[
.blockquote[
**Aur√©lien G√©ron**<br>
√â a ci√™ncia (e arte) de programar computadores de tal forma que eles aprendam a partir de dados `r Citep(myBib, "geron2017")`.
]
]

<!-- Slide XX -->
---
name: TA
class: inverse, middle, left
background-image: url(fig/white.jpg)
background-size: cover

.font180[**Tipos de Aprendizagem** <br> (*Supervisionada vs n√£o-supervisionada*)]

<!--Slide 8 -->
---
# Tipos de Aprendizagem

.pull-left-4[
.blockquote[
.content-box-blue[**Aprendizagem Supervisionada:**]

- **Objetivo principal**:

Dado as medidas $\left(x_1, y_1\right)$, . . . , $\left(x_n, y_n\right)$, o objetivo √© estimar uma fun√ß√£o $f(x)$ para predizer um novo valor de $y$ baseado em $x$.

- **Tipos de problemas**: classifica√ß√£o e regress√£o.

- **Vari√°vel resposta**: Existe (dados rotulados).
]]

--

.pull-right-4[
.blockquote[
.content-box-blue[**Aprendizagem n√£o-supervisionada:**]
- **Tipos de problemas**: agrupamento, redu√ß√£o de dimensionalidade e associa√ß√£o.
    - **Objetivo principal**: Descobrir estruturas subjacentes;
    - **Vari√°vel resposta**: N√£o existe (dados n√£o rotulados)
]]


<!--Slide 9 -->
---
name: tipos
# Tipos de Aprendizagem

```{r, echo=FALSE, out.width='80%', fig.align='center', fig.cap='', dpi=600}
knitr::include_graphics('fig/ML.png')
```
.left[.footnote[Fonte: O Autor.]]

<!--Slide 10 -->
---
# A id√©ia de "Supervisionar"...

.blockquote[
**Em um conjunto de dados...** <br>

(..) para cada observa√ß√£o de preditora(s), $x_{i}$ (i = 1, ..., n), existe uma resposta associada $y_{i}$ `r Citep(myBib, "james2013introduction", .opts = list(max.names = 1, longnamesfirst = F))`.
] <br>

\[
\normalsize
\underbrace{
\overbrace{\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1n} \\
x_{21} & x_{22} & \cdots & x_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & x_{n2} & \cdots & x_{nn}
  \end{bmatrix}
    }^{\mathbf{X}}
    }_{\mathbf{Preditoras}}
\underbrace{
\overbrace{
  \begin{bmatrix}
    y_1     \\
    y_2     \\
    \vdots  \\
    y_n     \\
  \end{bmatrix}
    }^{\mathbf{y}}
    }_{\mathbf{Resposta}}
\]

.blockquote[
**O papel de Supervisor...** <br>
A id√©ia √© que cada $y_{i}$ exer√ßa um papel de .content-box-red[‚ÄúProfessor‚Äù], respons√°vel por .content-box-red[‚ÄúSupervisionar‚Äù] o processo de aprendizagem de uma
fun√ß√£o $f(x)$ `r Citep(myBib, "hastie2016elements")`.
]

<!--Slide 11 -->
---
# Um conjunto de dados...

.pull-left[
.center[**Cubagem por Smalian**] 

.scroll-box-20[
```{r, echo=FALSE, eval=TRUE, message=FALSE}
library(kableExtra)
data <- data.table::fread("R-scripts/Cedrela.csv")

data %>% kable(format= "html") %>%
  kable_styling(full_width = F, bootstrap_options = c("striped", "hover", "condensed"), font_size = 18)
```
]]
.pull-right[
.center[**Terminologias**] 

.blockquote[
1. Comunidade de aprendizado de m√°quina:
- **Inst√¢ncias/exemplos**: dados individuais.
- **Atributos/caracter√≠sticas/features:**
    - Caracter√≠sticas mensur√°veis de uma inst√¢ncia.
- **Vari√°vel resposta/labels/classe**: 
    - Vari√°vel de interesse para predi√ß√£o.

*Tipos de atributos*: bin√°rios, categ√≥ricos, num√©ricos
]]

<!--Slide 12 -->
---
# At√© aqui...
<br>

.pull-left-2[
.blockquote[
- **Motivos para fazer regress√£o**: pr√°ticos e estat√≠sticos;

- **Duas Culturas no uso de ME**: DMC e .content-box-red[AMC];

- **Algorithmic modeling culture**: relaciona-se ao campo de .content-box-red[Aprendizado de M√°quina];

- **Tipos de Aprendizado**: .content-box-red[Supervisionado] e N√£o-Supervisionado;

- **Aprendizado Supervisionado**: estimar uma fun√ß√£o $f(x)$ para predizer um novo valor de $y$ baseado em $x$. Se $y$ for um n√∫mero real (n√∫merico) tem-se um .content-box-red[problema de regress√£o].
]
]<br>

.center[
.font150[
Quais algoritmos existentes para estimar $\Large{f\left(x\right)}$?
]
]

<!-- Slide XX -->
---
name: AAS
class: inverse, middle, left
background-image: url(fig/white.jpg)
background-size: cover

.font180[**Algoritmos de Aprendizado <br> Supersivionado**]

<!--Slide 13 -->
---
# Algoritmos de AS
**- Alguns dos principais algoritmos para problemas de regress√£o:**
.pull-left[
```{r, echo=FALSE, out.width='110%', fig.align='center', fig.cap='', dpi=600}
knitr::include_graphics('fig/Tec.png')
```
.left[.footnote[Fonte: O Autor.]]
]

--

.pull-rigth[
.center[**Pacote CARET** <br>

Mais de 200 algoritmos para aprendizado supervisionado.

`r Citep(myBib, c("kuhn2013applied", "R-caret"))` <br>

Web page: [CARET Package](https://topepo.github.io/caret/index.html)

]
]

<!--Slide 14 -->
---
name: RT
class: inverse, middle, left
background-image: url(fig/white.jpg)
background-size: cover

.font200[**Regression Tree**]

.font150[(.red[C]lassification .red[A]nd .red[R]egression .red[T]rees - CART)]

`r Citep(myBib, "breiman1984classification", .opts = list(max.names = 1, longnamesfirst = F))`

<!--Slide 15-->
---
# Regression Tree (CART)

.pull-left-1[
```{r, echo=FALSE, out.width='80%', fig.align='center', fig.cap='', dpi=600}
knitr::include_graphics('fig/CART-Book.jpg')
```
.center[Fonte: [www.amazon.com.br](https://www.amazon.com.br/Classification-Regression-Trees-Leo-Breiman/dp/0534980546)]
]

.pull-right-2[
**Informa√ß√µes gerais:**
- **CART**: algoritmo mais famoso;
- **Significado:** .red[C]lassification .red[A]nd .red[R]egression .red[T]rees;
- **Proposto**: `r Citet(myBib, "breiman1984classification", .opts = list(max.names = 1, longnamesfirst = F))`;
- **Problemas**: Classifica√ß√£o e Regress√£o; e
- **Hiperpar√¢metro tuning**: cp (par√¢metro de complexidade).

]

<!--Slide 16 -->
---
# Regression Tree (CART)

.pull-left-2[
.blockquote[
- **Qual a ideia principal:**

  - Realizar **particionamentos recursivos** no espa√ßo das covari√°veis, e ajustar um **modelo constante** da vari√°vel resposta em cada parti√ß√£o. <br>


- **Motivo**:
  - Encontrar parti√ß√£o em que as observa√ß√µes sejam mais **homog√™neas** o poss√≠vel para a **vari√°vel resposta**. <br>


- **Covari√°veis**: categ√≥ricas, dicot√¥micas, discretas ou cont√≠nuas.
]
]

<!--Slide 17 -->
---
# Regression Tree (CART)

.pull-left-3[
.blockquote[
**Estrutura:** <br>

1) **N√≥ raiz** (*root node*): $R_{1}$

2) **N√≥ intermedi√°rio** (*intermediate node*): $I_{2}$, $I_{3}$ e $I_{7}$

3) **N√≥ terminal** ou **folha** (*leaf* or *terminal node*): $T_{4}$, $T_{5}$, $T_{6}$, $T_{8}$ e $T_{9}$

4) **Divis√µes** (*splits*): $d_{1}$, $d_{2}$, $d_{3}$ e $d_{4}$ (condi√ß√µes "se-ent√£o")
]

--

.pull-bottom[
.blockquote[
- **Etapas do aprendizado**:

  1) Crescimento de uma √°rvore completa (adulta) at√© algum crit√©rio de parada; e
  
  2) Poda da √°rvore (overfitting).
]
]


]

.pull-right-3[
.center[**Uma √°rvore de regress√£o...**]

```{r, echo=FALSE, out.width='100%', fig.align='center', fig.cap='', dpi=600}
knitr::include_graphics('fig/CART1.png')
```
.left[.footnote[Fonte: Adaptado de Breiman et al. (1984).]]
]

<!--Slide 15 -->
---
# Regression Tree: Como uma √°rvore cresce?

.panelset[
.panel[.panel-name[Dados]
.pull-left[

.scroll-box-20[
```{r, echo=FALSE, eval=TRUE, message=FALSE}
data <- data.table::fread("R-scripts/Cedrela.csv")

data %>% kable(format= "html") %>%
  kable_styling(full_width = F, bootstrap_options = c("striped", "hover", "condensed"), font_size = 18)

```
]]
.pull-right[
**Meta**: Estimar uma fun√ß√£o de regress√£o $f(x)$ usando o algoritmo CART para predizer o volume $\left(\normalsize V = f(D,H)\right)$

**Foco**: Como √© o processo de crescimento de uma √°rvore CART?

]
]

.panel[.panel-name[CART]
.pull-left[
```{r, echo=FALSE, fig.width=5, fig.height=3, warning=FALSE, fig.align='center', fig.cap='√Årvore CART (rpart/cp = 0/minsplit = 20)', dpi=600, eval=TRUE, message=FALSE}
set.seed(1)
data2 <- data %>% select(-Arv)

tree <- rpart(V ~ ., 
              data = data2, 
              method="anova", 
              control = rpart.control(cp = 0, minsplit = 20))

rattle::fancyRpartPlot(tree,
                       yesno=2,
                       split.col="black",
                       nn.col="black",
                       caption="",
                       palette="Set2",
                       branch.col="black",
                       digits = 4)

# rpart.plot(tree, box.palette="RdBu", shadow.col="gray", nn=TRUE, digits = 4)
```
*minsplit* = M√≠nimo de 20 observa√ß√µes p/ tentar uma divis√£o
]

.pull-right[
.scroll-box-20[
.shadow[
**1) In√≠cio**
- N√≥ raiz (todas inst√¢ncias)/(**n = 32**)
- C√°lculos: 
  - M√©dia aritm√©tica da resposta (melhor ``chute''); 
  **5.724 m`r knitr::asis_output("\U00B3")`**
  
  - Soma de Quadrados de Res√≠duos (SQR). **326.19 (m`r knitr::asis_output("\U00B3")`)`r knitr::asis_output("\U00B2")`**

\begin{equation}
SQR = \sum_{i=1}^{n}(y_{i} - \hat{f_{m}}(x_{i}))^{2}
\end{equation}

$\hat{f_{m}}(x_{i})$: predi√ß√£o na regi√£o $m$ em que a i-√©sima observa√ß√£o √© alocada.
]

.shadow[
**2) Primeira divis√£o** 
- Qual divis√£o selecionar?
  - Maior redu√ß√£o da SQR: Qual vari√°vel e ponto de corte?
  
  1) Vari√°vel escolhida: **D (di√¢metro)**
  
  2) Ponto de corte: **95.97cm**
  
- Qual SQR p/ n√≥s filhos?
    - $SQR_{N_{2}}$: 
    **64.39 (m`r knitr::asis_output("\U00B3")`)`r knitr::asis_output("\U00B2")`**
    - $SQR_{N_{3}}$: 
    **93.88 (m`r knitr::asis_output("\U00B3")`)`r knitr::asis_output("\U00B2")`**
    
\begin{equation}
{\Delta R(s,t)} = R(t) - (R(t_L) + R(t_R)) \\
\end{equation}

 = **326.19280 - (64.39354 + 93.87834)**
 
 = **`r tree$frame$dev[1] - (tree$frame$dev[2] + tree$frame$dev[5])` (m`r knitr::asis_output("\U00B3")`)`r knitr::asis_output("\U00B2")`**

]

.shadow[
**3) Segunda divis√£o**
   
  1) Vari√°vel escolhida: **D (di√¢metro)**
  
  2) Ponto de corte: **79.74cm**
  
- Qual SQR p/ n√≥s filhos?
    - $SQR_{N_{4}}$: 
    **17.41 (m`r knitr::asis_output("\U00B3")`)`r knitr::asis_output("\U00B2")`**
    - $SQR_{N_{5}}$: 
    **13.90 (m`r knitr::asis_output("\U00B3")`)`r knitr::asis_output("\U00B2")`**
    
\begin{equation}
{\Delta R(s,t)} = R(t) - (R(t_L) + R(t_R)) \\
\end{equation}

 = **64.39354 - (17.40637 + 13.89782)**
 
 = **`r tree$frame$dev[2] - (tree$frame$dev[3] + tree$frame$dev[4])` (m`r knitr::asis_output("\U00B3")`)`r knitr::asis_output("\U00B2")`**

]   

.shadow[
.center[**Resumindo**]

O uso de cp=0 permitiu o crescimento de uma √°rvore sem restri√ß√µes, comumente chamada "√°rvore adulta". Tamb√©m, minsplit = 20 imp√¥s a condi√ß√£o de existir no m√≠nimo 20 observa√ß√µes em um n√≥ para que uma divis√£o fosse tentada.

- *Estrutura*: 
    - Duas divis√µes bin√°rias;
    - Tr√™s n√≥s terminais (*);
    - Quatro regras do tipo if-then (Se ent√£o); e
    - Somente a vari√°vel D (di√¢metro) foi escolhida.
]
  
]]]

.panel[.panel-name[Regras]

```{r, echo=FALSE, out.width='40%', fig.align='center', fig.cap='', dpi=600}
print(tree)
rpart.rules(tree, cover = TRUE)
```

]

.panel[.panel-name[SQR-N1]

.pull-left[

.scroll-box-20[
```{r, echo=FALSE, eval=TRUE, message=FALSE}
library(kableExtra)
data <- data.table::fread("R-scripts/Cedrela.csv")

data %>% 
  arrange(D) %>%
  mutate("yi-fm" = V-mean(data$V)) %>%
  mutate("(yi-fm)¬≤" = (V-mean(data$V))^2) %>%
  janitor::adorn_totals("row") %>%
  #rmarkdown::paged_table()
  kable(format= "html") %>%
  kable_styling(full_width = F, bootstrap_options =
                  c("striped", "hover", "condensed"), 
                font_size = 18) %>%
  row_spec(33, bold = T, 
           color = "white", background = "black") %>%
  row_spec(25, bold = T,
           color = "white", background = "orange") %>%
  row_spec(26, bold = T,
           color = "white", background = "orange")
```

]]

.pull-right[
- **1) In√≠cio**:

$\bar{V}$ = `r mean(data$V)`;

$SQR_{N_{1}}$ = `r sum((data$V-mean(data$V))^2)`

Ponto de corte: (95.49 + 96.45)/2 = **95.97cm**
]
]

.panel[.panel-name[SQR-N2-N3]

.pull-left[

.scroll-box-20[
```{r, echo=FALSE, eval=TRUE, message=FALSE}
df <- data %>% arrange(D) %>%
  filter(D < 95.97) %>%
  mutate("yi-fm" = V-mean(V)) %>%
  mutate("(yi-fm)¬≤" = (V-mean(V))^2)

df %>%
  janitor::adorn_totals("row") %>%
  kable(format= "html") %>%
  kable_styling(full_width = F, bootstrap_options =
                  c("striped", "hover", "condensed"),
                font_size = 18) %>%
  row_spec(26, bold = T,
           color = "white", background = "black") %>%
  row_spec(16, bold = T,
           color = "white", background = "orange") %>%
  row_spec(17, bold = T,
           color = "white", background = "orange")
```

]]

.pull-right[
```{r, echo=FALSE, eval=TRUE, message=FALSE}
df2 <- data %>% arrange(D) %>%
  filter(D >= 95.97) %>%
  mutate("yi-fm" = V-mean(V)) %>%
  mutate("(yi-fm)¬≤" = (V-mean(V))^2)

df2 %>% 
  janitor::adorn_totals("row") %>%
  kable(format= "html") %>%
  kable_styling(full_width = F, bootstrap_options =
                  c("striped", "hover", "condensed"),
                font_size = 18) %>%
  row_spec(8, bold = T,
           color = "white", background = "black")
```

.pull-bottom[
Ponto de corte: **79.74cm**

$SQR_{N_{2}}$ = `r sum((df$V-mean(df$V))^2)`;
$\bar{V}$ = `r mean(df$V)`

$SQR_{N_{3}}$ = `r sum((df2$V-mean(df2$V))^2)`;
$\bar{V}$ = `r mean(df2$V)`

]
]
]

.panel[.panel-name[SQR-N4-N5]

.pull-left[

.scroll-box-20[
```{r, echo=FALSE, eval=TRUE, message=FALSE}
df4 <- data %>% arrange(D) %>%
  filter(D < 79.74) %>%
  mutate("yi-fm" = V-mean(V)) %>%
  mutate("(yi-fm)¬≤" = (V-mean(V))^2) 

df4 %>%
  janitor::adorn_totals("row") %>%
  kable(format= "html") %>%
  kable_styling(full_width = F, bootstrap_options =
                  c("striped", "hover", "condensed"),
                font_size = 18) %>%
  row_spec(17, bold = T,
           color = "white", background = "black")
```

]

.pull-bottom[
Ponto de corte: **79.74cm**

$SQR_{N_{4}}$ = `r sum((df4$V-mean(df4$V))^2)`;
$\bar{V}$ = `r mean(df4$V)`
]
]

.pull-right[
```{r, echo=FALSE, eval=TRUE, message=FALSE}
df5 <- data %>% arrange(D) %>%
  filter(D >= 79.74 & D < 95.97) %>%
  mutate("yi-fm" = V-mean(V)) %>%
  mutate("(yi-fm)¬≤" = (V-mean(V))^2)

df5 %>%
  janitor::adorn_totals("row") %>%
  kable(format= "html") %>%
  kable_styling(full_width = F, bootstrap_options =
                  c("striped", "hover", "condensed"),
                font_size = 18) %>%
  row_spec(10, bold = T,
           color = "white", background = "black")
```

.pull-bottom[

$SQR_{N_{5}}$ = `r sum((df5$V-mean(df5$V))^2)`;
$\bar{V}$ = `r mean(df5$V)`

]
]
]

.panel[.panel-name[Regi√µes]

```{r, echo=FALSE, fig.align='center', fig.width=9, fig.height=3, fig.cap='', dpi=600, eval=TRUE, warning=FALSE, message=FALSE}
data <- data.table::fread("R-scripts/Cedrela.csv")

r1 <- data %>% 
  ggplot() + 
    geom_point(data=data, aes(x=D, y=V, color=ifelse(data$D >= tree$splits[1,4], "red", "black")), size=1) +
  geom_vline(xintercept=tree$splits[1,4], colour="red", linetype="dotted") +
  scale_colour_manual(values = c("black", "red")) +
  geom_text(aes(x=tree$splits[1,4], y=15, 
                label="D >= 95.97"), colour="red", 
            angle=90, vjust = 1.2, size = 4) +
  theme(legend.position="none")

r2 <- r1 + 
  geom_point(data=data, aes(x=D, y=V, colour=ifelse(D < tree$splits[3,4], "blue", ifelse(D >= tree$splits[1,4], "red", "black"))), size=1) +
  scale_colour_manual(values = c("blue", "black", "red"))+
  geom_vline(xintercept=tree$splits[3,4], colour="blue", linetype="dotted") +
    geom_text(aes(x=tree$splits[3,4], y=15, 
                label="D >= 79.74"), colour="blue", 
            angle=90, vjust = 1.2, size = 4) +
  theme(legend.position="none")

r3 <- r2 + 
  geom_text(aes(x=65, y=10, label="R4 \n Vest. = 3.65"), colour="black", 
            vjust = 1.2, size = 2) +
  geom_text(aes(x=87, y=10, label="R5 \n Vest. = 6.05"), colour="blue", 
            vjust = 1.2, size = 2) +
  geom_text(aes(x=123, y=10, label="R3 \n Vest. = 10.05"), colour="red", 
            vjust = 1.2, size = 2)
  
ggpubr::ggarrange(r1, r2, r3, ncol=3, 
                  labels=c("", ""),
                  font.label=list(size=30))

```

]

.panel[.panel-name[f(x)]
.pull-left-2[
```{r, echo=FALSE, fig.align='center', fig.width=5, fig.height=3, fig.cap='', dpi=600, eval=TRUE, warning=FALSE, message=FALSE}

r2 + geom_line(aes(x = D, y = predict(tree)))
  
```
]
]
]

<!-- Slide XX -->
---
# Underfitting e Overfitting
<br>
S√£o conceitos estat√≠sticos relacionados ao ajuste dos modelos.

- **Subajuste** (*Underfitting*)
  - Quando o modelo aprendido produz **predi√ß√µes ruins** no **conjunto de treinamento**, ou seja o modelo √© simples demais para explicar os dados usados no processo de aprendizado.

- **Sobreajuste** (*Overfitting*)
  - Quando o modelo aprendido produz **boas predi√ß√µes** no **conjunto de treinamento** (se ajusta demais aos dados de treino), por√©m **n√£o generaliza bem para novos dados**.

**Ideal**: Encontrar um modelo intermedi√°rio entre subajuste e superajuste!

<!-- Slide XX -->
---
# Underfitting e Overfitting
<br>
√Årvores de regress√£o t√™m tend√™ncia de sobreajustar.

.pull-left[
```{r, echo=FALSE, out.width='80%', fig.cap="Overfitting"}
bio <- data.table::fread("R-scripts/AGB.csv")

rpart2 <- rpart(AGB ~ D, 
                data = bio, 
                control = list(cp = 0, maxdepth = 10, 
                               minbucket = 1, minsplit = 5))

ggplot(data=bio, aes(D, AGB)) +
  geom_point(aes(D, AGB), alpha = .3, size = 2) +
  geom_line(aes(x = D, y = predict(rpart2)), 
            color = "red", size = 1) +
  ylab("Biomassa A√©rea Total (kg)") + 
  xlab("Di√¢metro (cm)")
```
]

.pull-right[
```{r, echo=FALSE, out.width='80%', fig.cap="Underfitting"}
rpart3 <- rpart(AGB ~ D, 
                data = bio, 
                control = list(cp = 0, maxdepth = 2, 
                               minbucket = 1, minsplit = 5))

ggplot(data=bio, aes(D, AGB)) +
  geom_point(aes(D, AGB), alpha = .3, size = 2) +
  geom_line(aes(x = D, y = predict(rpart3)), 
            color = "red", size = 1) +
  ylab("Biomassa A√©rea Total (kg)") + 
  xlab("Di√¢metro (cm)")
```
]

<!-- Slide XX -->
---
# Overfitting: como minimizar?
<!-- C√≥digo adaptado de Bradley boehmke. Dispon√≠vel em: link: https://github.com/bradleyboehmke/random-forest-training/blob/master/docs/slides-source.Rmd -->

.pull-left-4[
Duas estrat√©gias principais:

**1) Parada antecipada**

  - Restringir a profundidade m√°xima da √°rvore (maxdepth); e
  
  - Restringir o n√∫mero m√≠nimo de observa√ß√µes em qualquer n√≥ terminal (minbucket).

]

.pull-right-4[
```{r, echo=FALSE, out.width='100%', cache=TRUE}
library("gganimate")
library("gifski")
library("transformr")

bio <- data.table::fread("R-scripts/AGB.csv")
maxdepth <- 1:12
results <- data.frame(NULL)

for(i in maxdepth) {
 ctrl <- list(cp = 0, maxdepth = i, 
              minbucket = 1, minsplit = 5)
 fit <- rpart(AGB ~ D, data = bio, control = ctrl) 
 
 pred <- mutate(bio, maxdepth = maxdepth[i])
 pred$pred <- predict(fit, bio)
 results <- rbind(results, pred)
   
}

p <- results %>%
  ggplot(aes(D, pred)) +
  geom_point(aes(D, AGB), alpha = .3, size = 2) +
  geom_line(color = "red", size = 1) +
  ylab("Biomassa A√©rea Total (kg)") + 
  xlab("Di√¢metro (cm)") + 
  labs(title = 'Profundidade m√°xima: {frame_time}') +
  transition_time(maxdepth)
  animate(p, renderer = gifski_renderer(), device = "png")
```  

]

<!-- Slide XX -->
---
# Overfitting: como minimizar?

.pull-left-4[
Duas estrat√©gias principais:

**1) Parada antecipada**

  - Restringir a profundidade m√°xima da √°rvore (maxdepth); e
  
  - Restringir o n√∫mero m√≠nimo de observa√ß√µes em qualquer n√≥ terminal (minbucket).

]

.pull-right-4[
```{r, echo=FALSE, out.width='100%', cache=TRUE}
minbucket <- 1:20
results <- data.frame(NULL)

for(i in minbucket) {
 ctrl <- list(cp = 0, maxdepth = 10, 
              minbucket = i, minsplit = 5)
 fit <- rpart(AGB ~ D, data = bio, control = ctrl) 
 
 pred <- mutate(bio, minbucket = minbucket[i])
 pred$pred <- predict(fit, bio)
 results <- rbind(results, pred)
   
}

p <- results %>%
  ggplot(aes(D, pred)) +
  geom_point(aes(D, AGB), alpha = .3, size = 2) +
  geom_line(color = "red", size = 1) +
  ylab("Biomassa A√©rea Total (kg)") + 
  xlab("Di√¢metro (cm)") + 
  labs(title = 'N√∫mero m√≠nimo de observa√ß√µes no n√≥ terminal: {frame_time}') +
  gganimate::transition_time(minbucket)
  animate(p, renderer = gifski_renderer(), device = "png")
```  

]

<!-- Slide XX -->
---
# Overfitting: como minimizar?
<br>

Duas estrat√©gias principais:

.pull-left[
**2) Poda custo complexidade**

  - Cresce uma √°rvore adulta (complexa).
  
  - As parti√ß√µes s√£o sucessivamente desfeitas (podada) para encontrar uma sub√°rvore √≥tima; e
  
  - Fun√ß√£o de custo-complexidade: $R_{Œ±}(T) = R(T) + Œ±|T|$
]

.pull-right[

.blockquote[
.center[**Termos**]

  $T$: representa uma √°rvore;
  
  $|T|$: n√∫mero de n√≥s terminais;
  
  $R(T)$: soma de quadrados de res√≠duos; e
  
  $Œ±$: Par√¢metro custo complexidade (controla complexidade). Hiperpar√¢metro tuning.
  
  Se $Œ± = 0$ $\rightarrow$ a melhor √°rvore ser√° a maior √°rvore.

  **Valida√ß√£o cruzada**: Usada para encontrar o melhor $Œ±$ (a melhor sub√°rvore).
]
]

<!-- Slide 16 -->
---
name: bag
class: inverse, middle, left
background-image: url(fig/white.jpg)
background-size: cover

.font200[**Bagging**]

.font150[(.red[B]ootstrap .red[Agg]regat.red[ing])]

`r Citep(myBib, "breiman1996bagging", .opts = list(max.names = 1, longnamesfirst = F))`

<!-- Slide 17 -->
---
# Bagging

.pull-left-1[
```{r, echo=FALSE, out.width='80%', fig.align='center', fig.cap='', dpi=600}
knitr::include_graphics('fig/Breiman1996.png')
```
.center[Fonte: [link.springer.com](https://link.springer.com/article/10.1007/BF00058655)]

]

.pull-right-2[
**Informa√ß√µes gerais:**
- **Bagging**: um dos primeiros algoritmo de conjunto (ensemble);
- **Significado:** .red[B]ootstrap .red[Agg]regat.red[ing];
- **Proposto**: `r Citet(myBib, "breiman1996bagging")`;
- **Problemas**: Classifica√ß√£o e Regress√£o; e
- **Hiperpar√¢metro tuning**: nbagg (n√∫mero de replica√ß√µes de bootstrap);
]

<!--Slide 18 -->
---
# Bagging

.pull-left-2[
.blockquote[
- **Ideia principal:**

  - Ajustar m√∫ltiplas vers√µes de um modelo preditivo, e depois agreg√°-las, para obter uma √∫nica predi√ß√£o combinada `r Citep(myBib, "breiman1996bagging")`.<br>


- **Motivos**:
  - Reduzir a vari√¢ncia de um m√©todo de predi√ß√£o, como as √°rvores de regress√£o, e promover melhorias substanciais na acur√°cia `r Citep(myBib, "james2013introduction", .opts = list(max.names = 1, longnamesfirst = F))`.<br>
  
  
- **Aprediz de base**:
  - M√©todos de alta vari√¢ncia (e baixo vi√©s) (ex.: √°rvores de regress√£o). <br>
  
    <!-- - **Vantagens**: Ajuda a reduzir a varia√ß√£o, minimizar o overfitting (sobreajuste) e, por conseguinte, melhorar o desempenho preditivo. <br> -->
]
]
---
# M√©todo Bootstrap

.pull-left-3[
.blockquote[
√â um **m√©todo de reamostragem** que consiste em **produzir replica√ß√µes** de tamanho $n$, com mesmo tamanho do conjunto de aprendizado, por **amostragem aleat√≥ria com reposi√ß√£o** `r Citep(myBib, "gama2015extraccao", .opts = list(max.names = 1, longnamesfirst = F))`.
]
]

.pull-bottom[
```{r, echo=FALSE, out.width='70%', fig.align='center', fig.cap='', dpi=600}
knitr::include_graphics('fig/Bootstrap.png')
```
.footnote[Fonte: Adaptado de [texample.net](https://texample.net/tikz/examples/bootstrap-resampling/)
]

]

<!--Slide XX -->
---
# Bagging (Bagged Trees)

.pull-left-2[
.blockquote[
O bagging pode ser usado para combinar **√°rvores modelos** `r Citep(myBib, "witten2017data", .opts = list(max.names = 1, longnamesfirst = F))`, e mais comumente **√°rvores de decis√£o** `r Citep(myBib, "james2013introduction", .opts = list(max.names = 1, longnamesfirst = F))`.
]
]

--

.pull-right-2[
**Cada √°rvore crescida possui alta vari√¢ncia, mas baixo vi√©s.**
.blockquote[
**No caso de regress√£o (usando √°rvores)...**

**Passo 1** - Dado um conjunto de treinamento original $L$ de tamanho $n$, gerar uma amostra *bootstrap* $bi$ de tamanho $n$;

**Passo 2** - Treinar um modelo de √°rvore de regress√£o completamente crescida (profundas e n√£o podadas) usando a amostra *bootstrap* $bi$; e

**Passo 3** - Salvar o modelo preditivo.

Esse procedimento pode ser realizado $B$ vezes. Em que: $B$ = n√∫mero de amostras bootstrap.
]
]

<!--Slide XX -->
---
# Bagging (Bagged Trees)

```{r, echo=FALSE, out.width='100%', fig.align='center', fig.cap='', dpi=600}
knitr::include_graphics('fig/Bagg.png')
```

\begin{equation}
{\hat{f}_{(bag)}(x)} = \frac{1}{B}\displaystyle\sum\limits_{b=1}^B\hat{f}^{*b}(x)
\label{eq:avBAG}
\end{equation}

<!--Slide 19 -->
---
# Bagging (Bagged Trees)

.panelset[
.panel[.panel-name[Dados]
.pull-left-1[

.scroll-box-20[
```{r, echo=FALSE, eval=TRUE}
data <- data.table::fread("R-scripts/Cedrela.csv")

data %>% kable(format= "html") %>%
  kable_styling(full_width = F, bootstrap_options = c("striped", "hover", "condensed"), font_size = 18)

```

]]

.pull-right-2[
**Implementa√ß√£o no R** (exemplo de brinquedo!)

```{r, echo=TRUE, eval=TRUE}
data <- data.table::fread("R-scripts/Cedrela.csv")

set.seed(1)
bagTr <- ipred::bagging(V ~ D + H,
                 data = data, 
                 nbagg = 6, # (Recomenda√ß√£o: 50-250)
                 coob = F,  # out-of-bag estimate
                 control = rpart.control(
                   cp = 0, 
                   minsplit = 20,
                   maxdepth = 30))
```

**Out-of-Bag**: observa√ß√µes n√£o sorteadas na amostra bootstrap $b_{i}$. Portanto, n√£o usadas para aprender a $f(x)$.

**Out-Of-Bag estimate**: √â o erro m√©dio de predi√ß√£o calculado usando as amostras Out-of-Bag.

]
]

.panel[.panel-name[Bagging]

.pull-left-5[
```{r, echo=FALSE, eval=TRUE, fig.width=7, fig.height=2}
par(mfrow = c(1, 6))

nbag <- 1:length(bagTr$mtrees)

for(i in nbag) {
rpart.plot(bagTr$mtrees[[i]]$btree, 
           box.palette="RdBu", 
           shadow.col="gray", 
           nn=TRUE, 
           digits = 4,
           main = paste0("√Årvore: ", i)
           )
}

```
]
]

.panel[.panel-name[Dados]

]

]


<!--Slide 20 -->
---
# Regression Tree: pontos fortes e fracos...
.shadow[
- **Pontos fortes**:
  - White model;
  - Extremamente intuitivo e f√°cil de interpretar; (regras "se-ent√£o");
  - Predi√ß√µes s√£o simples (m√©dia, m√©dia ponderada);
  - N√£o exige c√°lculos complexos;
  - F√°cil identificar as vari√°veis que contribuem p/ a resposta;
  - Lidam facilmente com valores ausentes, e estes n√£o afetam consideralvelmente a constru√ß√£o da √°rvore (n√£o h√° necessidade de imputar os valores ausentes);
  - Sele√ß√£o autom√°tica de vari√°veis importantes;
  - Admite vari√°veis qualitativas (sem necessidade de transforma√ß√µes); e
  - Vari√°veis num√©ricas: n√£o requer normaliza√ß√£o.
]
--

<br>
.shadow[
- **Pontos fracos**:
    - Muito inst√°vel (alta vari√¢ncia): 
    Uma pequena mudan√ßa nos dados pode implicar em uma grande mudan√ßa na estrutura (diferentes regras "se-ent√£o") da √°rvore de decis√£o.
    
]

<!--Slide 16 -->
---
# Iniciar apresenta√ß√£o de algumas t√©cnicas
- Antes, fazer uma introdu√ß√£o informando a ideia de predizer
a f(x), e portanto existente in√∫meras t√©cnicas de AM para isso...Obviamente, falar que a primeira ideia √© fazer RL...
- Usar base de dados de nativa (falar com Bruno), selecionar algumas √°rvores...
- Inserir uma figura com as principais t√©cnicas de IA
- No CART:
 . Iniciar: mostrando uma √°rvore e explicando seus elementos;
 . mostrar o passo a passo das divis√µes bin√°rias (figura);
 . Detalhar o calculo matem√°tico, baseando-se nos dados usados
 para criar a √°rvore mostrada...

---
# .font80[Percep√ß√µes e reflex√µes para Mensura√ß√£o Florestal]

**Sob a √≥tica preditivista**:

**1 - Competitividade**: AM tem mostrado competitividade frente aos m√©todos tradicionais para modelar rela√ß√µes dendrom√©tricas;

**2 - Superioridade**: Existem relatos de maior acur√°cia para modelos de AM frente √† RL. Mas, estes ainda s√£o pontuais/poucos, e algumas vezes os ganhos de acur√°cia n√£o parecem ser expressivos.

**3. Princ√≠pio da parcim√¥nia**: Nestes casos (item 2), invocar o princ√≠pio da parcim√¥nia (escolher o modelo mais simples) parece ser mais plaus√≠vel.

**4 - Covari√°veis tradicionais**: Usar apenas as covari√°veis tradicionais (ex.: D e H para modelar o volume) para estimar modelos de AM parece n√£o produzir resultados extremamente superiores do que os m√©todos tradicionais (ex.: Schumacher-Hall). Sobretudo, em florestas equi√¢neas, em FNI ainda faltam mais estudos. <br>


**Mais recursos podem melhorar o desempenho preditivo de modelos aprendidos usando a cultura de modelagem algor√≠tmica.**

<!--Slide 13 -->
---
name: AS
# Aprendizado Supervisionado

```{r, echo=FALSE, out.width='80%', fig.align='center', fig.cap=''}
knitr::include_graphics('fig/intuicao-ML.png')
```

.left[.footnote[Fonte: O Autor.]] 

---
# Avalia√ß√£o de modelos

---
# Underfitting e Overfitting

---
# Divis√£o de dados

.blockquote[
Em geral, na literatura de AM reporta-se a tr√™s tipos de conjuntos de dados `r Citep(myBib, "witten2017data", .opts = list(max.names = 1, longnamesfirst = F))`:

- **Conjunto de aprendizado** (treino) 
    - Usado para construir os modelos preditivos (classifica√ß√£o ou regress√£o).
    
    
- **Conjunto de valida√ß√£o**
    - Usado para otimizar os hiperpar√¢metros dos modelos e escolher a configura√ß√£o de melhor desempenho preditivo.
    

- **Conjunto de teste**
  - Usado para estimar o desempenho preditivo final do modelo otimizado (tuning model). 
    
]
---
# M√©todos de Reamostragem

.blockquote[

- **Hold-out validation**
    - Separa 2/3 para treinamento e 1/3 para teste.
    
- **k-fold Cross-Validation**
    - Particiona o conjunto de treino em *k* subconjuntos
disjuntos (*k*-1), e testa no conjunto de valida√ß√£o (hold-out).
    
- **Leave-one-out Cross-Validation**
  - k=n

- **Bootstrap**
  - Amostragem com reposi√ß√£o.
]


<!--Slide6 -->
---
name: MAM
# Um modelo de aprendizado de m√°quina: CART

```{r, echo=FALSE, out.width='80%', fig.align='center', fig.cap=''}
knitr::include_graphics('fig/CART.png')
```

.left[.footnote[Fonte: O Autor.]] 

<!-- Slide XX -->
---
class: inverse, middle, left
background-image: url(fig/white.jpg)
background-size: cover

.font250[**Aplica√ß√µes nas ci√™ncias <br> florestais**]

<!-- Slide XX -->
---
name: aplicacoes
# Aplica√ß√µes nas ci√™ncias florestais

```{r, echo=FALSE, out.width='57%', fig.align='center', fig.cap=''}
knitr::include_graphics('fig/MindMap.png')
```

.left[.footnote[Fonte: O Autor.]]

<!-- Slide XX -->
---
name: AMIF
# .font80[AM na Mensura√ß√£o Florestal: estudos indicam potencial!]

```{r, echo=FALSE, out.width='80%', fig.align='center', fig.cap='', dpi=600}
knitr::include_graphics('fig/ML-TEC.png')
```
.left[.footnote[Fonte: O Autor.]]

```{r}
Citep(myBib, c("diamantopoulou2010modelling", "nieto2012support", "sanquetta2013use", "diamantopoulou2005artificial", "ozccelik2010estimating", "schikowski2015estudo", "nunes2016artificial", "Corte2020", "montano2017artificial", "korhonen1997application", "maltamo2001most", "fehrmann2008comparison", "sanquetta2015comparison", "sanquetta2018volume", "souza2019",
               "james2013introduction"))
```

<!-- Slide XX -->
---
class: inverse, middle, left
background-image: url(fig/white.jpg)
background-size: cover

.font200[**Frameworks para aprendizado <br> de m√°quina**]

<!-- Slide XX -->
---
name: AMLR
class: middle
# Aprendizado de m√°quina com linguagem R

.pull-left[

Alguns pacotes publicados no Cran Task View: Machine Learning & Statistical learning

```{r, echo=FALSE, out.width='85%', fig.align='center', fig.cap='', dpi=600}
knitr::include_graphics('fig/ML-FR.png')
```
]

.pull-right[
<BR> <BR> <BR> <BR> <BR>
Veja outros pacotes dispon√≠veis em Task View:

[Machine Learning & Statistical learning](https://cran.r-project.org/)

<BR> <BR>
Web scraping feito em 21/07/2018:

At√© julho de 2018 existiam 102 pacotes sobre AM
publicados no CRAN Task View.
]

.left[.footnote[Fonte: O Autor.]]

---
name: caret
class: inverse, left
background-image: url(fig/white.jpg)
background-size: cover

.font300[.thick[CARET]]
	
.font200[(.red[C]lassication .red[A]nd .red[Re]gression .red[T]raining)]
	
`r Citep(myBib, c("kuhn2013applied", "R-caret"))`.
		
.font120["Constitui um conjunto de fun√ß√µes que tentam simplificar <br> o processo de constru√ß√£o de modelos preditivos."]

<br> <br> <br>

Web page: [CARET Package](https://topepo.github.io/caret/index.html)

<br> <br>

.left[.footnote[Imagem de <a href="https://pixabay.com/pt/users/pen_ash-5526837/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5358904">pen_ash</a> por <a href="https://pixabay.com/pt/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5358904">Pixabay</a>]]
---
# Pacote CARET: Ferramentas

```{r, echo=FALSE, out.width='50%', fig.align='center', fig.cap='', dpi=600}
knitr::include_graphics('fig/CARET.png')
```

---
name: fluxo
background-image: url(fig/white.jpg)
background-size: cover

# Fluxograma - Constru√ß√£o de modelos de AM usando CARET

```{r, echo=FALSE, out.width='90%', fig.align='center', fig.cap=''}
knitr::include_graphics('fig/FM.png')
```
.left[.footnote[Fonte: `r Citet(myBib, "souza2020Tese")`.]]

---
name: exemploR
class: inverse, top
background-image: url(fig/white.jpg)
background-size: cover

<br> <br> <br> <br> <br>
.pull-left-3[
MACHINE LEARNING NO R

<br> <br>

Acesse o R-script:

[01-wkNN](https://deivisonsouza.github.io/IA-Potencial-IF/R-scripts/01-wkNN.R)

Acesse os dados:

[Tectona-grandis](https://deivisonsouza.github.io/IA-Potencial-IF/R-scripts/Tectona.csv)]

.footnote[Cr√©ditos: Imagem de .segue-yellow[Willas Lima].]
]
---
name: shiny
class: inverse, top
background-image: url(fig/white.jpg)
background-size: cover

Como disponibilizar os modelos de aprendizado de m√°quina?

.pull-left[
```{r, echo=FALSE, out.width='20%', fig.align='center', fig.cap=''}
knitr::include_graphics('fig/sh.png')
```

.thick[.black[Biblioteca Shiny]]

.black[Biblioteca que facilita a cria√ß√£o de aplicativos da Web interativos usando diretamente a Linguagem R.]

.thick[.black[Shinyapps.io:]]

.black[Servidor gratuito para hospedar os aplicativos Shiny.]
<BR>

.thick[.black[Acesse aplica√ß√µes Web para predi√ß√£o de biomassa e volume: (Souza, 2020)]]

.font80[
[1-Shiny-MLMBio-Biomassa](https://deivisonsouza.shinyapps.io/MLBiomass/)

[2-Shiny-MLMVol-Volume](https://deivisonsouza.shinyapps.io/MLVolume/)
]
]

.column-right[
.thick[.black[Acesse a base para uso na aplica√ß√£o MLMBio:]] [Biomass-data](https://deivisonsouza.github.io/IA-Potencial-IF/R-scripts/data.csv)]

]

---
# Refer√™ncias
<br>

```{r refs1, echo=FALSE, results="asis"}
PrintBibliography(myBib, start = 1, end = 5)
```
---
# Refer√™ncias
<br>

```{r refs2, echo=FALSE, results="asis"}
PrintBibliography(myBib, start = 6, end = 10)
```
---
# Refer√™ncias
<br>

```{r refs3, echo=FALSE, results="asis"}
PrintBibliography(myBib, start = 11, end = 15)
```
---
# Refer√™ncias
<br>

```{r refs4, echo=FALSE, results="asis"}
PrintBibliography(myBib, start = 16, end = 19)
```

---
class: inverse, top
background-image: url(fig/white.jpg)
background-size: cover

.pull-left[
.font300[Obrigado!]
 <br> <br> <br>

**Email**: <a href="mailto:deivisonvs@ufpa.br">deivisonvs@ufpa.br</a>

**Github**: <a href="https://github.com/DeivisonSouza">@DeivisonSouza</a>

<br>

.shadow[
Os slides, c√≥digos e demais arquivos desta apresenta√ß√£o est√£o dispon√≠veis no reposit√≥rio GitHub: [ML-Mensuracao-Florestal](https://github.com/DeivisonSouza/ML-Mensuracao-Florestal).
]
]

<!-- # Modelagem preditiva: linguagens (softwares) e frameworks -->

<!-- ```{r, engine='tikz', cache=TRUE, echo=FALSE, out.width='50%', fig.ext = 'png', bg = "transparent"} -->
<!-- \usetikzlibrary{mindmap} -->
<!-- \begin{tikzpicture} -->
<!-- 		[mindmap, -->
<!-- 		every node/.style={concept, execute at begin node=\hskip0pt, font=\bfseries}, -->
<!-- 		root concept/.append style={concept color=black, fill=white, line width=1ex, text=black, font=\huge\bfseries}, -->
<!-- 		method a/.style={concept color=red, line width=1ex, text=white}, -->
<!-- 		method b/.style={concept color=orange, line width=1ex, text=white}, -->
<!-- 		method c/.style={concept color=brown, line width=1ex, text=white}, -->
<!-- 		methods a/.style={concept color=red!50, line width=1ex, text=black}, -->
<!-- 		methods b/.style={concept color=orange!50, line width=1ex, text=black}, -->
<!-- 		methods c/.style={concept color=brown!50, line width=1ex, text=black}, -->
<!-- 		grow cyclic, -->
<!-- 		level 1/.append style={level distance=4.5cm, sibling angle=60}, -->
<!-- 		level 2/.append style={level distance=3cm, sibling angle=45}] -->
<!-- 		\node [root concept] {Liguagens/ \\ Software} % root -->
<!-- 		child [missing] {} -->
<!-- 		child [missing] {} -->
<!-- 		child [missing] {} -->
<!-- 		child [method a] { node {Python} -->
<!-- 			child [methods a] { node {A1} } -->
<!-- 			child [methods a] { node {A2} } -->
<!-- 			child [methods a] { node {A3} } -->
<!-- 			child [methods a] { node {A4} } -->
<!-- 			child [methods a] { node {A5} } -->
<!-- 			child [methods a] { node {A6} } -->
<!-- 			child [methods a] { node {A7} } } -->
<!-- 		child [missing] {} -->
<!-- 		child [method b] { node {R} -->
<!-- 			child [methods b] { node {B1} } -->
<!-- 			child [methods b] { node {B2} } -->
<!-- 			child [methods b] { node {B3} } -->
<!-- 			child [methods b] { node {B4} } -->
<!-- 			child [methods b] { node {B5} } -->
<!-- 			child [methods b] { node {B6} } -->
<!-- 			child [methods b] { node {B7} } } -->
<!-- 		child [missing] {} -->
<!-- 		child [method c] { node {Weka} -->
<!-- 			child [methods c] { node {C1} } -->
<!-- 			child [methods c] { node {C2} } -->
<!-- 			child [methods c] { node {C3} } -->
<!-- 			child [methods c] { node {C4} } -->
<!-- 			child [methods c] { node {C5} } -->
<!-- 			child [methods c] { node {C6} } -->
<!-- 			child [methods c] { node {C7} } }; -->
<!-- 		\end{tikzpicture} -->
<!-- ``` -->





